id,type,question,answer,options
1,MCQ,Does Spark handle storage directly?,No  it relies on external storage (e.g. HDFS S3),Yes;No;Only for small datasets
1,Fill,Spark's __________ API allows reading/writing data from external storage.,DataFrameReader/DataFrameWriter,
1,TrueFalse,Spark stores data permanently in its executors' memory.,FALSE,
1,Scenario,"Your Spark job fails with ""No space left on device"". What's the first check?",Verify external storage (e.g., S3/HDFS) availability
1,MCQ,Which storage systems integrate with Spark?,"HDFS, S3, ADLS, GCS",HDFS;S3;ADLS;GCS;All
2,MCQ,What is the primary goal of partitioning in Spark?,Distribute data evenly across executors,Faster UI;Smaller logs;Even data distribution
2,Fill,"For a 1TB dataset, use __________ partitions as a starting point.",200,
2,TrueFalse,More partitions always improve performance.,FALSE,
2,Scenario,Your Spark job has 10 tasks; 9 finish in 1min, 1 takes 1hr. How to fix?,Repartition to address skew
2,Fill,Use __________ to inspect partition sizes.,df.rdd.mapPartitionsWithIndex(),
3,MCQ,How to remove duplicates in PySpark?,dropDuplicates(),distinct();dropDuplicates();filter()
3,Fill,"To remove nulls, use df.__________().",na.drop(),
3,TrueFalse,df.distinct() is faster than dropDuplicates() for large datasets.,FALSE,
3,Scenario,Duplicate records appear after a join. How to prevent?,Use distinct keys or dropDuplicates post-join,
3,MCQ,Which method removes both duplicates AND nulls?,df.na.drop().dropDuplicates(),distinct();dropDuplicates();na.drop() combo
4,MCQ,What causes OOM errors in Spark?,Insufficient executor memory,Small data;Insufficient memory;Too few partitions
4,Fill,"To fix OOM, increase __________ or reduce __________.",executor memory;partition size,
4,TrueFalse,Setting `spark.sql.shuffle.partitions=1000` always prevents OOM.,FALSE,
4,Scenario,OOM occurs during a groupBy. How to optimize?,Increase partitions or use reduceByKey,
4,MCQ,Which config parameter directly affects executor memory?,spark.executor.memory,spark.driver.memory;spark.executor.memory;spark.shuffle.partitions
5,MCQ,What is the default broadcast join threshold in Spark?,10MB,1MB;10MB;100MB
5,Fill,"To force a broadcast join, use __________.",broadcast() hint,
5,TrueFalse,Broadcast joins work well for large tables.,FALSE,
5,Scenario,Broadcast join fails due to table size. What's the alternative?,Sort-merge join with partitioning,
5,MCQ,Which join type minimizes shuffling?,Broadcast join,Broadcast;Sort-merge;Nested loop
6,MCQ,What is Delta Lake's primary advantage over Parquet?,ACID transactions,Faster reads;ACID;Smaller files
6,Fill,"To enable schema evolution in Delta, set __________.",spark.databricks.delta.schema.autoMerge.enabled=true,
6,TrueFalse,Delta Lake time travel requires manual version snapshots.,FALSE,
6,Scenario,A pipeline breaks after a source schema change. How to fix Delta tables?,Use `MERGE` or autoMerge,
6,MCQ,Which command shows Delta table history?,DESCRIBE HISTORY,DESCRIBE DETAIL;DESCRIBE HISTORY;SHOW VERSIONS
7,MCQ,What is the purpose of Unity Catalog?,Data governance and access control,Job scheduling;Data governance;Cost tracking
7,Fill,"To grant table access in Unity Catalog, use __________.",GRANT SELECT,
7,TrueFalse,Unity Catalog only works with Delta tables.,FALSE,
7,Scenario,A user can't query a table. How to debug in Unity Catalog?,Check GRANT permissions,
7,MCQ,Which object is NOT managed by Unity Catalog?,Raw JSON files,Delta tables;SQL warehouses;Raw JSON files
8,MCQ,How to trigger a Databricks notebook from another?,dbutils.notebook.run(),%run;dbutils.notebook.run();magic.run
8,Fill,"To pass parameters between notebooks, use __________.",dbutils.widgets,
8,TrueFalse,Notebook triggers work across different Databricks workspaces.,FALSE,
8,Scenario,Notebook B must run only if Notebook A succeeds. How to implement?,Use try/except with dbutils.notebook.run(),
8,MCQ,Which magic command runs a notebook inline?,%run,%execute;%run;%include
9,MCQ,What is the replication factor in HDFS?,3 (by default),1;3;5
9,Fill,HDFS __________ detects failed DataNodes via heartbeat.,NameNode,
9,TrueFalse,HDFS replication guarantees zero data loss.,FALSE,
9,Scenario,A DataNode fails. How does HDFS recover?,Replicate blocks from other nodes,
9,MCQ,Which HDFS component stores metadata?,NameNode,DataNode;NameNode;YARN
10,MCQ,What is Kafka's partition reassignment used for?,Rebalancing load across brokers,Adding topics;Rebalancing;Deleting data
10,Fill,"To monitor Kafka lag, check __________.",consumer group offsets,
10,TrueFalse,Kafka partitions are immutable after creation.,FALSE,
10,Scenario,Consumer lag spikes. How to troubleshoot?,Check consumer throughput or add partitions,
10,MCQ,Which tool manages Kafka partition reassignment?,kafka-reassign-partitions.sh,zookeeper;kafka-topics;kafka-reassign-partitions
11,MCQ,What is the primary purpose of predicate pushdown in Spark?,Filter data at the storage level before loading,Reduce network I/O;Filter early;Both
11,Fill,"To enable predicate pushdown for Parquet files, set __________.",spark.sql.parquet.filterPushdown=true,
11,TrueFalse,Predicate pushdown works with all file formats (e.g., CSVJSON).,FALSE
11,Scenario,Your query reads 100GB but only needs 1GB. How to optimize?,Use partition pruning + predicate pushdown,
11,MCQ,Which operation benefits MOST from predicate pushdown?,SELECT with WHERE clause,GROUP BY;JOIN;SELECT with WHERE
12,MCQ,What is the default shuffle partition count in Spark?,200,100;200;300
12,Fill,"To reduce shuffle overhead, set __________.",spark.sql.shuffle.partitions,
12,TrueFalse,Increasing shuffle partitions always improves performance.,FALSE,
12,Scenario,A join operation takes hours due to shuffling. How to fix?,Optimize partition count + broadcast if possible,
12,MCQ,Which config controls shuffle partition count?,spark.sql.shuffle.partitions,spark.shuffle.partitions;spark.sql.shuffle.partitions;spark.default.parallelism
13,MCQ,What is the key advantage of Delta Lake over raw Parquet?,ACID transactions,Smaller files;ACID;Faster reads
13,Fill,"To time-travel in Delta Lake, use __________.",VERSION AS OF,
13,TrueFalse,Delta Lake supports batch and streaming writes.,TRUE,
13,Scenario,A pipeline fails; how to revert Delta table to last good version?,Use `RESTORE TABLE TO VERSION`,
13,MCQ,Which command lists Delta table versions?,DESCRIBE HISTORY,SHOW VERSIONS;DESCRIBE HISTORY;LIST SNAPSHOTS
14,MCQ,How does Kafka partition reassignment help?,Balances load across brokers,Fixes lag;Balances load;Deletes data
14,Fill,"To reassign Kafka partitions manually, use __________.",kafka-reassign-partitions.sh,
14,TrueFalse,Kafka partitions can ONLY be increased, not decreased.,FALSE
14,Scenario,Consumer lag spikes due to uneven partitions. Solution?,Reassign partitions + monitor,
14,MCQ,Which tool monitors Kafka consumer lag?,kafka-consumer-groups.sh,kafka-topics;kafka-console-producer;kafka-consumer-groups
15,MCQ,What is the difference between RANK() and ROW_NUMBER()?,RANK() leaves gaps for ties,ROW_NUMBER is unique;RANK leaves gaps;Both
15,Fill,"To find the 3rd highest salary, use __________.",DENSE_RANK(),
15,TrueFalse,WINDOW functions require PARTITION BY.,FALSE,
15,Scenario,Query needs top 5 sales reps per region. How to implement?,Use `ROW_NUMBER() OVER(PARTITION BY region)`,
15,MCQ,Which function assigns consecutive ranks without gaps?,DENSE_RANK(),RANK();DENSE_RANK();ROW_NUMBER()
16	MCQ	What is the primary purpose of coalesce() in Spark?	Reduce partitions without full shuffle	Increase partitions;Reduce partitions;Change data
16	Fill	To reduce from 1000 to 100 partitions, use __________.	coalesce(100)	
16	TrueFalse	coalesce() always avoids a full shuffle.	TRUE	
16	Scenario	Your DataFrame has 200 uneven partitions. How to optimize?	Use repartition(100) for balanced data	
16	MCQ	Which operation performs a full shuffle?	repartition()	coalesce();repartition();both
17	MCQ	What is the difference between managed and external tables in Spark?	Managed tables drop data on table deletion	Managed tables control lifecycle;External tables don't;Both
17	Fill	To create an external table, use __________.	CREATE EXTERNAL TABLE	
17	TrueFalse	Managed tables are always better for production.	FALSE	
17	Scenario	You need to share data between Spark and Hive. Which table type?	External table	
17	MCQ	Where is metadata stored for external tables?	External metastore	Hive metastore;External metastore;Nowhere
18	MCQ	What is the key advantage of Spark over MapReduce?	In-memory processing	Faster disks;In-memory processing;Better UI
18	Fill	Spark's __________ API improved performance over RDDs.	DataFrame	
18	TrueFalse	Spark always runs faster than MapReduce.	FALSE	
18	Scenario	Your MapReduce job takes 8 hours. How would Spark help?	Use in-memory caching of intermediate data	
18	MCQ	Which Spark feature reduces disk I/O?	Caching intermediate results	More executors;Caching;Smaller data
19	MCQ	What is the purpose of Delta Lake's __delta_log folder?	Stores transaction logs and versions	Data files;Transaction logs;Both
19	Fill	To vacuum Delta Lake files older than 7 days, use __________.	VACUUM RETAIN 7 DAYS	
19	TrueFalse	Delta_log files can be safely deleted manually.	FALSE	
19	Scenario	Your Delta table grows too large. How to compact small files?	Run OPTIMIZE command	
19	MCQ	Which command checks Delta table file statistics?	DESCRIBE DETAIL	SHOW FILES;DESCRIBE DETAIL;LIST FILES
20	MCQ	What is the primary use case for Kafka Streams?	Real-time stream processing	Batch processing;Real-time streams;Both
20	Fill	To process Kafka streams in Spark, use __________.	Structured Streaming	
20	TrueFalse	Kafka Streams requires Spark for processing.	FALSE	
20	Scenario	You need millisecond-latency processing of clickstream data. Solution?	Kafka Streams with stateful operations	
20	MCQ	Which library provides exactly-once semantics for Kafka?	Kafka Streams	Spark Streaming;Kafka Streams;Flink
21	MCQ	What is the difference between UNION and UNION ALL?	UNION removes duplicates	UNION ALL faster;UNION removes duplicates;Same
21	Fill	To combine datasets keeping duplicates, use __________.	UNION ALL	
21	TrueFalse	UNION is more expensive than UNION ALL.	TRUE	
21	Scenario	You need to merge 3 datasets with possible duplicates. Which operation?	UNION ALL + distinct()	
21	MCQ	Which operation performs deduplication automatically?	UNION	UNION;UNION ALL;Neither
22	MCQ	What is the purpose of normalization in databases?	Reduce data redundancy	Improve joins;Reduce redundancy;Both
22	Fill	The 3rd normal form eliminates __________ dependencies.	transitive	
22	TrueFalse	Denormalization never improves performance.	FALSE	
22	Scenario	Your queries are slow due to excessive joins. How to optimize?	Consider selective denormalization	
22	MCQ	Which normal form deals with partial key dependencies?	2NF	1NF;2NF;3NF
23	MCQ	What is the primary advantage of Parquet over CSV?	Columnar storage for better compression	Smaller size;Columnar storage;Both
23	Fill	To write a DataFrame in Parquet format, use __________.	df.write.parquet()	
23	TrueFalse	Parquet is always better than Delta Lake.	FALSE	
23	Scenario	Your analytics queries only need 2 columns from 100. Best format?	Parquet	
23	MCQ	Which feature makes Parquet efficient for analytics?	Column pruning	Row slicing;Column pruning;Compression
24	MCQ	What is the purpose of the NameNode in HDFS?	Manages metadata and file system tree	Stores data;Manages metadata;Both
24	Fill	If a DataNode fails, __________ replicates its blocks.	NameNode	
24	TrueFalse	NameNode stores actual file data.	FALSE	
24	Scenario	Your HDFS cluster has frequent DataNode failures. How to improve?	Increase replication factor	
24	MCQ	Which HDFS component stores file data?	DataNode	NameNode;DataNode;Both
25	MCQ	What is the primary use of OLAP systems?	Analytical processing	Transactions;Analytics;Both
25	Fill	For fast aggregations, use __________ databases.	OLAP	
25	TrueFalse	OLTP systems optimize for read-heavy workloads.	FALSE	
25	Scenario	You need to analyze 5 years of sales data. Which system?	OLAP (e.g.	 Snowflake  Redshift)
25	MCQ	Which database type typically uses star schemas?	OLAP	OLTP;OLAP;Both
26	MCQ	What is the primary advantage of using Delta Live Tables (DLT)?	Automated pipeline maintenance	Manual tuning;Automated maintenance;Simpler syntax
26	Fill	To declare a DLT pipeline, use __________ syntax.	CREATE LIVE TABLE	
26	TrueFalse	DLT pipelines automatically handle schema evolution.	TRUE	
26	Scenario	Your team needs reliable ETL with minimal manual intervention. Solution?	Implement Delta Live Tables	
26	MCQ	Which DLT feature helps with data quality?	Expectations	Assertions;Expectations;Validation
27	MCQ	What is the purpose of tumbling windows in streaming?	Fixed-size	 non-overlapping time intervals Event counting;Fixed intervals;Sliding intervals
27	Fill	To aggregate 5-minute counts in Spark Streaming, use __________.	window(timeColumn, '5 minutes')	
27	TrueFalse	Tumbling windows can have overlapping intervals.	FALSE	
27	Scenario	You need hourly sales totals from a stream. Which window type?	Tumbling window (1 hour)	
27	MCQ	Which window type allows overlapping intervals?	Sliding window	Tumbling;Sliding;Session
28	MCQ	What is the key difference between narrow and wide transformations?	Narrow transformations don't require shuffling	Shuffle requirement;Data locality;Both
28	Fill	Examples of narrow transformations include __________.	filter(), map()	
28	TrueFalse	reduceByKey() is a narrow transformation.	FALSE	
28	Scenario	Your Spark job has excessive shuffling. How to optimize?	Replace wide transformations where possible	
28	MCQ	Which transformation always causes shuffling?	join()	map();filter();join()
29	MCQ	What is the primary purpose of the Catalyst Optimizer?	Query optimization	Memory management;Query optimization;Network I/O
29	Fill	To view Spark's optimized query plan, use __________.	EXPLAIN EXTENDED	
29	TrueFalse	The Catalyst Optimizer works only with RDDs.	FALSE	
29	Scenario	Your SQL query runs slowly. How to investigate?	Check the optimized physical plan	
29	MCQ	Which Spark component converts DataFrame operations to optimized RDDs?	Catalyst Optimizer	Spark Core;Catalyst;YARN
30	MCQ	What is the purpose of Change Data Feed in Delta Lake?	Track row-level changes	Version control;Row-level changes;Both
30	Fill	To enable Change Data Feed, set __________.	delta.enableChangeDataFeed=true	
30	TrueFalse	Change Data Feed requires separate storage.	FALSE	
30	Scenario	You need to identify changed rows since yesterday. Solution?	Query Change Data Feed	
30	MCQ	Which command reads Change Data Feed?	TABLE_CHANGES	DESCRIBE HISTORY;TABLE_CHANGES;VERSION
31	MCQ	What is the primary use of mapping data flows in ADF?	Code-free data transformations	Orchestration;Transformations;Monitoring
31	Fill	To debug a mapping data flow, use __________.	data flow debug mode	
31	TrueFalse	Mapping data flows run on Spark clusters.	TRUE	
31	Scenario	Your team needs GUI-based ETL without coding. Solution?	ADF mapping data flows	
31	MCQ	Which ADF component handles complex transformations?	Mapping data flows	Wrangling flows;Mapping;Both
32	MCQ	What is the purpose of the medallion architecture?	Data quality progression (bronzeâ†’gold)	Storage tiers;Quality progression;Both
32	Fill	The __________ layer contains cleansed, business-ready data.	gold	
32	TrueFalse	All data should skip the silver layer.	FALSE	
32	Scenario	Your lakehouse has raw	 inconsistent data. How to structure?	Implement medallion architecture
32	MCQ	Which layer contains raw  unprocessed data?	Bronze 	Bronze;Silver;Gold
33	MCQ	What is the primary advantage of Spark Structured Streaming?	Unified API for batch and streaming	Lower latency;Unified API;Simpler setup
33	Fill	To read from Kafka in Structured Streaming, use __________.	readStream.format('kafka')	
33	TrueFalse	Structured Streaming provides exactly-once semantics.	TRUE	
33	Scenario	You need to process both historical and real-time data. Solution?	Structured Streaming	
33	MCQ	Which streaming mode processes data in micro-batches?	Structured Streaming	Kafka Streams;Structured;Flink
34	MCQ	What is the purpose of the Unity Catalog in Databricks?	Centralized governance and discovery	Security;Governance;Both
34	Fill	To share a table across workspaces, use __________.	Delta Sharing	
34	TrueFalse	Unity Catalog only works with Databricks SQL.	FALSE	
34	Scenario	Your organization needs centralized data access controls. Solution?	Implement Unity Catalog	
34	MCQ	Which Unity Catalog feature enables fine-grained access control?	Row-level security	Column masking;Row-level;Both
35	MCQ	What is the primary use of dbutils in Databricks?	Workspace utilities and file operations	Notebook workflows;Utilities;Both
35	Fill	To mount an S3 bucket, use __________.	dbutils.fs.mount()	
35	TrueFalse	dbutils can trigger jobs across cloud platforms.	FALSE	
35	Scenario	You need to access cloud storage from multiple notebooks. Solution?	Create mount points with dbutils	
35	MCQ	Which dbutils module handles notebook workflows?	notebook	fs;notebook;secrets
36	MCQ	What is the key advantage of using MERGE in Delta Lake?	Upsert capability (insert+update)	Faster deletes;Upserts;Both
36	Fill	For incremental upserts, use __________ syntax.	MERGE INTO target USING source	
36	TrueFalse	MERGE operations are atomic in Delta Lake.	TRUE	
36	Scenario	You need to synchronize a Delta table with daily changes. Solution?	Daily MERGE job	
36	MCQ	Which Delta Lake operation combines insert and update?	MERGE	INSERT;UPDATE;MERGE
37	MCQ	What is the purpose of the Delta Lake VACUUM command?	Remove unused files older than retention	Compact files;Remove old;Both
37	Fill	To set a 30-day file retention policy, use __________.	SET TBLPROPERTIES ('delta.logRetentionDuration'='30 days')	
37	TrueFalse	VACUUM immediately deletes all old files.	FALSE	
37	Scenario	Your Delta table storage grows uncontrollably. Solution?	Run VACUUM with retention policy	
37	MCQ	Which command optimizes Delta file size?	OPTIMIZE	VACUUM;OPTIMIZE;COMPACT
38	MCQ	What is the primary use of the Hive metastore?	Metadata management for tables	Data storage;Metadata;Both
38	Fill	To connect Spark to an external Hive metastore, set __________.	spark.sql.hive.metastore.version	
38	TrueFalse	Hive metastore is required for Spark SQL.	FALSE	
38	Scenario	You need to share table definitions between Spark and Hive. Solution?	Configure shared metastore	
38	MCQ	Which component stores Hive table schema information?	Metastore database	Namenode;Metastore;DataNode
39	MCQ	What is the key advantage of using CTEs in SQL?	Improve query readability and organization	Performance;Readability;Both
39	Fill	To reference a CTE multiple times, use __________.	WITH clause	
39	TrueFalse	CTEs materialize results for reuse.	FALSE	
39	Scenario	Your complex SQL query has repeated subqueries. How to simplify?	Replace with CTEs	
39	MCQ	Which SQL feature is similar to CTEs but materialized?	Temporary views	Subqueries;Temporary views;CTEs
40	MCQ	What is the purpose of predicate pushdown in Spark?	Filter data at source before processing	Network optimization;Early filtering;Both
40	Fill	To disable predicate pushdown for a query, use __________.	spark.sql.parquet.filterPushdown=false	
40	TrueFalse	Predicate pushdown works with all file formats.	FALSE	
40	Scenario	Your query reads 100GB but only needs 1GB. How to optimize?	Enable predicate pushdown	
40	MCQ	Which operation benefits most from predicate pushdown?	SELECT with WHERE clause	GROUP BY;JOIN;SELECT
